{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "random.seed(1111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images: 485\n",
      "0 images number: 181\n",
      "1 images number: 304\n"
     ]
    }
   ],
   "source": [
    "csv_path = os.path.join(\"../datas/labels.csv\")\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "image_all, label_all = np.array(df['image']), np.array(df['label'])\n",
    "data_list = [(int(image), int(label)) for image, label in zip(image_all, label_all)]\n",
    "\n",
    "print(f\"Total number of images: {len(data_list)}\")\n",
    "print(f\"0 images number: {np.sum(label_all == 0)}\")\n",
    "print(f\"1 images number: {np.sum(label_all == 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images number: 411\n",
      "Val images number: 74\n"
     ]
    }
   ],
   "source": [
    "data_list_0 = [data for data in data_list if data[1] == 0]\n",
    "data_list_1 = [data for data in data_list if data[1] == 1]\n",
    "random.shuffle(data_list_0)\n",
    "random.shuffle(data_list_1)\n",
    "\n",
    "# split train and val\n",
    "split_idx_0 = int(len(data_list_0) * 0.8)\n",
    "data_train_0, data_val_0 = data_list_0[:split_idx_0], data_list_0[split_idx_0:]\n",
    "\n",
    "split_idx_1 = len(data_val_0) # for balanced validation set\n",
    "data_train_1, data_val_1 = data_list_1[split_idx_1:], data_list_1[:split_idx_1]\n",
    "\n",
    "train_datas, val_datas = [], []\n",
    "for datas in [data_train_0, data_train_1]:\n",
    "    for data in datas:\n",
    "        train_datas.append(data)\n",
    "random.shuffle(train_datas)        \n",
    "\n",
    "for datas in [data_val_0, data_val_1]:\n",
    "    for data in datas:\n",
    "        val_datas.append(data)\n",
    "random.shuffle(val_datas)\n",
    "\n",
    "print(f\"Train images number: {len(train_datas)}\")\n",
    "print(f\"Val images number: {len(val_datas)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data split json file\n",
    "train_list = []\n",
    "val_list = []\n",
    "\n",
    "for idx, label in train_datas:\n",
    "    train_list.append({\n",
    "        \"color_path\": f\"color/{idx}.png\",\n",
    "        \"depth_path\": f\"depth/{idx}.png\",\n",
    "        \"label\": label\n",
    "    })\n",
    "for idx, label in val_datas:\n",
    "    val_list.append({\n",
    "        \"color_path\": f\"color/{idx}.png\",\n",
    "        \"depth_path\": f\"depth/{idx}.png\",\n",
    "        \"label\": label\n",
    "    })\n",
    "    \n",
    "data_dict = {\n",
    "    \"train_list\": train_list,\n",
    "    \"val_list\": val_list,\n",
    "    \"test_list\": val_list # test_list is the same as val_list in this case\n",
    "}\n",
    "\n",
    "with open(\"../datas/data_split.json\", \"w\") as f:\n",
    "    json.dump(data_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ys_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06b312d4da2a9a3686a6d52820f5105a519faf7cd6cc067e3b3e5e11d5973e41"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
